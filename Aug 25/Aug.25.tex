\documentclass{book}
\usepackage{physics}
\usepackage{amsmath}
\begin{document}
\chapter{Matrix Representations}
In the last lecture, we introduced kets, bras and operators. In this Chapter, we will show that they can be represented by column vectors, row vectors and matrices respectively. This is possible due to the closure on completeness relation. 
i.e. $I = \sum^N_{i=1} \ket{a^(i)} \bra{a^(i)} (i=1,2,...,N)$
Identify operator is the sum of each and every orthonormal eigenket and corresponding eigenbra, where orthonormal means
$$ \braket{a^(i)}{a^(i)} = \delta_{ij}$$
where $\delta$ is the dirac delta function such that
$$ \delta_{ij} = 
\begin{cases}
1, i = j \\
0, i \neq j
\end{cases}
$$
e.g. In spin-$\frac{1}{2}$ systems,
$$ I = \ket{+}\bra{+} + \ket{-}\bra{-}$$
where 
$$ \braket{+}{+} = 1, \braket{+}{-} = \braket{-}{+} = 0, \braket{-}{-} = 1$$
these results can be obtained by representing the vectors as
$$\ket{+} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$$ 
$$\ket{-} = \begin{bmatrix} 0 \\ 1 \end{bmatrix} $$
corresponds to
$$ \bra{+} = \begin{bmatrix} 1 & 0 \end{bmatrix} $$
$$\bra{-} = \begin{bmatrix} 0 & 1 \end{bmatrix} $$

\section{Inner Products of Bras and Kets}
$$ \braket{+}{+} = \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = 1 x 1 + 0 x 0 = 1 $$
$$ \braket{+}{-} = \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = 1 x 1 + 0 x 0 = 1 $$
\section{Outer Products of Bras and Kets}
$$\ket{+} \bra{+} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}  \begin{bmatrix} 1 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$$
$$\ket{+} \bra{-} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}  \begin{bmatrix} 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$$
$$ I = \ket{+} \bra{+} + \ket{+} \bra{-} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} + \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} $$
We can extrapolate this to an N-dimensional system with column vectors $\ket{a_1}$, $\ket{a_2}$ ... $\ket{a_N}$
$$\ket{a_1} = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$$
$$\ket{a_2} = \begin{bmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$$
and so on for up to $\ket{a_N}$ where each column vector has N entries. The corresponding bras or row vectors are
$$\bra{a_1} = \begin{bmatrix} 1, & 0, & \hdots & 0 \end{bmatrix}$$
$$\bra{a_2} = \begin{bmatrix} 0, & 1, & 0, & \hdots & 0 \end{bmatrix}$$
These vectors form a complete N-dimensional vector space and we can write any other vector as a sum of coefficients multiplied by our orthogonal basis
$$ \ket{\alpha} = \sum^N C_N\ket{a_N} = \sum^N \ket{a_N} \bra{a_N} \ket{\alpha} $$
\section{Operators as Outerproducts of Bras and Kets}
$$X=\ket{\alpha} \bra{\beta}$$
$\ket{\beta} = \ket{\alpha}$ if $\braket{\beta}{\beta} = 1$ (normalized) \\
because $X\ket{\beta} = \ket{\alpha}\bra{\beta}\ket{\beta}$
$$X = \begin{bmatrix}
	\alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_N
\end{bmatrix}
\begin{bmatrix}
	\beta_1*, & \beta_3* &  \hdots & \beta_N
\end{bmatrix}
=
\begin{bmatrix}
	\alpha_1\beta_1* & \alpha_1\beta_2* & \hdots & \alpha_1\beta_N* \\
	\alpha_2\beta_1* & \alpha_2\beta_2* & \hdots & \alpha_2\beta_N* \\
	\vdots & \vdots &  \ddots \\
	\alpha_N\beta_1* & \alpha_N\beta_2* & \hdots & \alpha_N\beta_N
\end{bmatrix}
$$
This operation created a N x N matrix
The trace of a matrix or Tr is the sum of the elements on the diagonal on the matrix for this matrix;
$$Tr X = \alpha_1\beta_1* + \alpha_2\beta_2* + \hdots + \alpha_N \beta_N $$
it then follows that
$$If X =\ket{\alpha}\bra{\beta}, then Tr X = \braket{\beta}{\alpha}$$
Adjoint Operator
$$X^\dagger = ket{\beta}\bra{\alpha} if X = \ket{\alpha}\bra{\beta}$$
$\bra{\beta} X^\dagger$ is dual correspondent to $X\ket{\beta} = \ket{\alpha}$ \\
($X^\dagger$ operator changes $\bra{\beta}$ to $\bra{\alpha}$ in bra space) \\
$$X^\dagger = 
\begin{bmatrix}
   \beta_1 \\ \beta_2 \\ \vdots \\ \beta_N
\end{bmatrix}
\begin{bmatrix}
   \alpha_1*, & \alpha_2* & \hdots, & \alpha_N*
\end{bmatrix}
=
\begin{bmatrix}
  \beta_1\alpha_1* & \beta_1\alpha_2* & \hdots & \beta_1\alpha_N* \\
   \beta_2\alpha_1* & \beta_2\alpha_2* & \hdots & \beta_2\alpha_N* \\
   \vdots & \vdots & & \vdots \\
   \beta_N\alpha_1* & \beta_N\alpha_2* & \hdots & \beta_N\alpha_N* \\
\end{bmatrix}
$$
Transpose of X is denoted by 
$$X^T = 
\begin{bmatrix}
   \alpha_1\beta_1* & \alpha_2\beta_1* &  \hdots & \alpha_N\beta_1* \\
   \alpha_1\beta_2* & \alpha_2\beta_2* & \hdots & \alpha_N\beta_2* \\
   \hdots & \hdots & & \hdots \\
   \alpha_1\beta_N & \alpha_2\beta_N & \vdots & \alpha_N\beta_N
\end{bmatrix}
$$
Thus, $ X^\dagger = (X^T)*$
i.e. The adjoint operator is equivalent to taking the transpose and then the complex conjugate
$$(X^\dagger)_ij = X*_ji$$
Note also
$$X_ij = \bra{a^i} X \ket{a^j} = \bra{a^i}\ket{\alpha}\bra{\betaâ€¢}\ket{a^j} = \alpha_i\beta_j^*$$
$$(X^\dagger)_ij = \ket{a^i} X^\dagger \ket{a^j} = \bra{a^j} X \ket{a^i}^* = X_ji^*$$
In general,
$$ \bra{\alpha} (X_1, X_2, ... , X_N) \ket{\beta}^* = \bra{\beta} X_N^\dagger, X_N ^\dagger, ... , X_2^\dagger X_1^\dagger \ket{\alpha} $$
If $X = X^\dagger$, then X is self-adjoint or X is a Hermitian Operator
$$X_ij = X_ji^* or X_ij^* = X_ji$$
Thus, the diagonal elements of Hermitian Operator are reall as $X_ii = X_ii^*$
It implies that the eigenvalues of Hermitian Operators are real. Observables are Hermitian Operators for this reason. Consider and observable A e.g. A could be $S_z$ or any spin observable.
$$ A\ket{a^i} = a^i\ket{a^i} (i = 1, 2, \hdots, N) $$
$$ A = A \cdot{} I = A ( \sum^N_{i=1} \ket{a^i}\bra{a^i}) = \sum^N_{i=1} A \ket{a^i}\bra{a^i}$$
$$ = \sum^N_{i=1} a^i \ket{a^i} \bra{a^i} = 
\begin{bmatrix}
   a^1 & & & \text{\large 0} \\
     & a^2 \\
    & & \ddots	\\
     \text{\large 0} & & & a^N
\end{bmatrix}
$$
\end{document}
